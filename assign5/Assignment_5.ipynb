{"cells":[{"cell_type":"markdown","metadata":{"id":"RnHeMdwQ93z0"},"source":["# Follow these instructions:\n","\n","Once you are finished, ensure to complete the following steps.\n","\n","1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n","\n","2.  Fix any errors which result from this.\n","\n","3.  Repeat steps 1. and 2. until your notebook runs without errors.\n","\n","4.  Submit your completed notebook to OWL by the deadline."]},{"cell_type":"markdown","metadata":{"id":"SK2gGiAt93z2"},"source":["# Assignment 6: Model Selection and Cross-validation [ __ /100 marks]\n","\n","\n","In this assignment we will examine [\"Forest Fires\"](https://archive.ics.uci.edu/ml/datasets/Forest+Fires) dataset to predict the burned area of forest fires giving some features. We will apply model selection and cross-validation method we learned."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"_B3mtq4793z3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.metrics import make_scorer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","np.set_printoptions(precision=3)\n","seed=0"]},{"cell_type":"markdown","metadata":{"id":"cPmWsHcg93z4"},"source":["## Question 1.0 [ _ /6 marks]\n","\n","Read the file `forestfires.csv` into a dataframe. Display the first 5 rows of this dataframe. "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ES-5D9MV93z5"},"outputs":[{"name":"stdout","output_type":"stream","text":["   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n","0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n","1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n","2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n","3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n","4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0\n"]}],"source":["# Read forestfires.csv into a dataframe [ /1 marks] \n","df = pd.read_csv('forestfires.csv')\n","\n","# Display the first 5 rows of the dataframe [ /1 marks]\n","print(df.head())"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"mltXl2HI93z6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows: 517\n","Number of null entries: 0\n","X          int64\n","Y          int64\n","month     object\n","day       object\n","FFMC     float64\n","DMC      float64\n","DC       float64\n","ISI      float64\n","temp     float64\n","RH         int64\n","wind     float64\n","rain     float64\n","area     float64\n","dtype: object\n"]}],"source":["# Inspect the data types of the attributes in the dataframe and answer the question in the next cell\n","\n","# Number of rows in the dataframe [ /1 marks]\n","print(\"Number of rows: \" + str(df.shape[0]))\n","\n","# Number of null entries in the dataframe [ /1 marks]\n","print(\"Number of null entries: \" + str(df.isnull().sum().sum()))\n","\n","# Types of all the columns (variables) in a dataframe [ /2 marks ]\n","print(df.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"4xlCQx1m93z7"},"source":[" **Questions**:\n"," 1. How many rows are there?  [ /1 marks]\n"," 2. Does the data consist of any null entries? [ /1 marks]\n"," 3. What categorical attributes do you see? [ /2 marks]\n"," \n","**Your answer**:\n","1. 517\n","2. No, there are zero null entries.\n","3. month and day are both categorical attributes."]},{"cell_type":"markdown","metadata":{"id":"RmcfdRLt93z8"},"source":["## Question 1.1 [ _ /15 marks]\n","\n","Using a threshold of statistical significance of 5%, check statistical significance for the labels of each categorical attribute. Group insignificant labels into two new statistically significant classes."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"K3W38eT593z-"},"outputs":[],"source":["# Check statistical significance of the labels in month. [ /3 marks]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VU4ecuwW930A"},"outputs":[],"source":["# Check statistical significance of labels in categorical attribute 2. [ /3 marks]\n","# ****** your code here ******\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_DLOKZT930B"},"outputs":[],"source":["# Group insignificant labels into two new statistically significant labels. [ /8 marks]\n","# ****** your code here ******\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxXYs8gL930C"},"outputs":[],"source":["# Recheck statistical significance of the attribute with adjusted labels [ /1 marks]\n","# ****** your code here ******\n"]},{"cell_type":"markdown","metadata":{"id":"cdWmbV9s930C"},"source":["## Question 1.2 [ _ /4 marks]\n","\n","Let's convert all categorical data into numerical data using `get_dummies`. Display the first 5 rows of your new dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_YgPiXP930C"},"outputs":[],"source":["# Use \"get_dummies\" to perform one hot encoding to the categorical attributes [ /3 marks]\n","# ****** your code here ****** \n","\n","\n","# Display first 5 rows of the dataframe [ /1 mark]\n","# ****** your code here ****** \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lxh1ba_F930C"},"outputs":[],"source":["# The .head() in the previous cell might give a truncated view. You can see all columns names using: \n","df.columns"]},{"cell_type":"markdown","metadata":{"id":"_fbDMIUM930C"},"source":["## Question 1.3 [ _ /8 marks]\n","\n","Let's examine the distribution of the target variable \"area\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KC2oXlFf930D"},"outputs":[],"source":["# Plot the distribution of target variable \"area\" with bins = 20 using an appropriate seaborn function [ /1 mark]\n","# ****** your code here ****** \n"]},{"cell_type":"markdown","metadata":{"id":"ZlWomIk2930E"},"source":[" **Question**:\n"," \n"," Describe the distribution of the target variable. We will use log transform on it, explain why would it help. [ /3 mark]\n","\n","**Your answer**:\n","\n","Here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijeAgmgm930E"},"outputs":[],"source":["# Use np.log to transform \"area\". [ /3 mark]\n","# Note that log is not defined every where and you might need to do something about it.\n","# ****** your code here ****** \n","\n","\n","# Plot the distribution of target variable \"area\" with bins = 20 using an appropriate seaborn function [ /1 mark]\n","# ****** your code here ****** \n"]},{"cell_type":"markdown","metadata":{"id":"wy-2cbp4930F"},"source":["## Question 1.4 [ _ /6 marks]\n","\n","Let's use **mean squared error** as our score metric. We can use `sklearn.metrics.mean_squared_error`, but here let's write our own function called `mse` with arguments `y` and `ypr`(predicted y) which returns the mean squared error. Recall the formula for MSE below:\n","\n","$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n}  \\left( \\hat{y_{i}}-y_{i}\\right)^{2} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRrjLAzO930G"},"outputs":[],"source":["# Define a function that takes in y's and returns MSE [ /6 marks]\n","# ****** your code here ****** \n","def mse():\n","    "]},{"cell_type":"markdown","metadata":{"id":"95Oo3pHa930G"},"source":["## Question 1.5 [ _ /4 marks]\n","\n","We will use all available features as predictors, and use the log transformed \"area\" as target variable. Then let's split our data into training and test. As usual, let's use test_size=0.2 and random_state=seed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNGlC1dT930G"},"outputs":[],"source":["# Create X and y [ /2 marks]\n","# ****** your code here ****** \n","X = \n","y = \n","\n","# Use train_test_split on X, y [ /2 marks]\n","# ****** your code here ****** \n"]},{"cell_type":"markdown","metadata":{"id":"Xg06M4Kn930H"},"source":["## Question 1.6 [ _ / 6 marks]\n","\n","For our first model, create a pipeline called \"M1\" that performs only a linear regression. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4kQncow930H"},"outputs":[],"source":["# Create a pipeline for model 1 (M1) [ /6 marks]\n","# ****** your code here ****** \n"]},{"cell_type":"markdown","metadata":{"id":"Pvcp2ZV1930I"},"source":["## Question 1.7 [ _ / 8 marks]\n","\n","For our second model let's add quadratic terms for all features (use `PolynomialFeatures`). Create a model pipeline for our second model (M2)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVFawDMO930I"},"outputs":[],"source":["# Create a pipeline for model 2 (M2) [ / 8 marks]\n","# ****** your code here ****** \n"]},{"cell_type":"markdown","metadata":{"id":"JWN9_NwU930I"},"source":["## Question 1.8 [ _ / 18 marks]\n","\n","`Temperature (temp)` and `Rain (rain)` may be important features, so let's extend model 1 by adding a *cubed* term for temp and a *squared* term for rain. Before creating a pipeline for this model, we need a custom transformer: we can specify a column for squared rain and one for cubed temp. The transformer has been initialized below, but you'll need to complete it with adding 1 or 2 lines of code. After this, create your corresponding pipeline (M3)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LfLjUml930I"},"outputs":[],"source":["# Modify the transform method of the KeyFeatures class [ /10 marks]\n","class KeyFeatures(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return\n","\n","    def transform(self, X, y=None):\n","        # ****** your code here ****** \n","        \n","        return\n","\n","# Create a pipeline for model 3 (M3) [ /8 marks]\n","# ****** your code here ****** \n"]},{"cell_type":"markdown","metadata":{"id":"YeLu0zzD930J"},"source":["## Question 1.9 [ _ /8 marks]\n","\n","For models 1-3, use 4-fold Cross-validation and report the mean and std of the loss (i.e., the `mse` function you created for Q1.4). For the cross-validation part, use `from sklearn.metrics import make_scorer` to make a scorer out of your `mse` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ue5BVAnk930J"},"outputs":[],"source":["# Use 4-fold CV on all models to get mean and std of score [ /8 marks]\n","# ****** your code here ****** \n","\n","\n","print(f\"M1 loss: %.4f +/- %.4f\" % (cvsc1.mean(), cvsc1.std()))\n","print(f\"M2 loss: %.4f +/- %.4f\" % (cvsc2.mean(), cvsc2.std()))\n","print(f\"M3 loss: %.4f +/- %.4f\" % (cvsc3.mean(), cvsc3.std()))"]},{"cell_type":"markdown","metadata":{"id":"8AnE8ms7930J"},"source":["## Question 2.0 [ _ / 3 marks]"]},{"cell_type":"markdown","metadata":{"id":"9V7YBbb6930K"},"source":["**Question**: \n","\n","Which model would you choose and why? [ /3 marks]\n","\n","**Your answer**:\n","\n","Here..."]},{"cell_type":"markdown","metadata":{"id":"Y-JSS9dp930K"},"source":["## Question 2.1 [ _ /  6 marks]\n","\n","Estimate the performance of your chosen model on the test data (which has been held out) using `mse`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBgAUuJm930L"},"outputs":[],"source":["# Compute the test loss on the unseen (test) dataset [ /6 marks]\n","# ****** your code here ******\n","\n","\n","print('MSE Loss on test data:',)"]},{"cell_type":"markdown","metadata":{"id":"PzgXRyPi930L"},"source":["## Question 2.2 [ _ /8 marks]\n","\n","Recap: The central limit theorem (CLT) states that if you have a population with mean $\\mu$ and standard deviation $\\sigma$ and take sufficiently large random samples from the population with replacement, then the distribution of the sample means will be approximately normally distributed. This will hold true regardless of whether the source population is normal or skewed, provided the sample size is sufficiently large (usually greater than 30).\n","\n","Compute (and print) a 95% confidence interval for the average test error using the Central Limit Theorem. You can use the following formula to compute it: \n","\n","$$ \\bar{L_n} \\pm 1.96 * \\frac{\\sigma_{l}}{\\sqrt{n}}$$\n","\n","Here $\\bar{L_n}$ is the average test loss (i.e. for our test set), $\\sigma_l$ is the standard deviation (of our test losses), and $n$ is the total number of test losses we compute.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgbNTtaC930L"},"outputs":[],"source":["# Test loss here is a point estimate (statistic) for the generalization error\n","# Having >30 samples, we can use the formula above safely\n","# Here we compute confidence interval for generalization error (i.e.expected [average] test loss for this particular dataset)\n","\n","# Calculate the 95% Confidence Interval for average test loss [ /8 marks]\n","# ****** your code here ****** \n","\n","\n","print('Confidence Interval is:', ci)"]},{"cell_type":"markdown","metadata":{"id":"UX_M8H7Z930L"},"source":["# Follow these instructions:\n","\n","Once you are finished, ensure to complete the following steps.\n","\n","1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n","\n","2.  Fix any errors which result from this.\n","\n","3.  Repeat steps 1. and 2. until your notebook runs without errors.\n","\n","4.  Submit your completed notebook to OWL by the deadline."]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 ('my_env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"vscode":{"interpreter":{"hash":"3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"}}},"nbformat":4,"nbformat_minor":0}
